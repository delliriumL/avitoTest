# Восстановление пробелов в русском тексте

## Контекст

В пользовательском вводе на Авито и подобных платформах часто встречаются строки без пробелов:  
`книгавхорошемсостоянии`, `купитьайфон14про`, `сдамквартиру`.  

Для человека такие тексты ещё читаемы, но для алгоритмов поиска и анализа — это серьёзная проблема:  
- снижается точность поиска,  
- усложняется работа рекомендательных систем,  
- ухудшается качество задач NLP (например, извлечение сущностей или классификация).  

Задача: построить модель, которая принимает текст без пробелов и возвращает восстановленный текст с корректными разделителями.  

---

## Рассмотренные подходы

### 1. Правила и словари  
Простейший вариант - использовать n-граммы, словари и динамическое программирование.  
Проблема в том, что:  
- русский язык слишком разнообразен,  
- опечатки и новые слова ломают словарный подход,  
- качество оказывается недостаточным.  

### 2. Токенизаторы (BPE, WordPiece)  
Токенизаторы умеют делить текст на подслова, но они не предназначены для восстановления именно пробелов.  
Разбиение получается непоследовательным и не совпадает с исходной сегментацией.  

### 3. Тяжёлые модели (BERT, T5 и др.)  
Современные языковые модели дают высокую точность, но:  
- требуют больших ресурсов для обучения и инференса,  
- избыточны для задачи уровня «расставить пробелы».  

### 4. BiLSTM (выбранное решение)  
Рекуррентные сети, особенно двунаправленные, хорошо подходят для задач последовательной разметки.  
- Они учитывают контекст как слева, так и справа от символа.  
- Работают на уровне символов, поэтому устойчивы к опечаткам и редким словам.  
- Архитектура лёгкая, обучение и инференс быстрые даже на CPU.  

**Вывод:** BiLSTM - это разумный компромисс между качеством и эффективностью.  

---

## Архитектура решения

1. **Подготовка данных**  
   - очищаем тексты,  
   - режем их на предложения (`razdel`),  
   - формируем пары:  
     - `no_spaces` - строка без пробелов,  
     - `labels` — бинарный вектор, где 1 = «пробел после символа».

2. **Векторизация**  
   - строим словарь символов,  
   - кодируем строки в последовательности индексов.  

3. **DataLoader**  
   - выполняем паддинг последовательностей,  
   - используем маски, чтобы считать loss только по валидным символам.  

4. **Модель**  
   - `Embedding` слой,  
   - двухслойный BiLSTM,  
   - линейный классификатор для каждого символа.  

5. **Обучение**  
   - функция потерь: `BCEWithLogitsLoss` с учётом дисбаланса классов,  
   - оптимизатор: `AdamW`,  
   - lr-scheduler: cosine annealing,  
   - подбор порога классификации по F1.  

6. **Инференс**  
   - модель выдаёт вероятности для каждого символа,  
   - бинаризация по оптимальному порогу,  
   - функция `restore_by_indices` возвращает итоговый текст.  

---

## Метрики

- Основная метрика - **F1-score**, так как важно одновременно:  
  - не ставить лишних пробелов (precision),  
  - не пропускать нужные (recall).  
- Порог классификации подбирается по валидации.  

---

## Результаты

- BiLSTM показала хорошее качество на валидации.  
- Модель лёгкая, работает быстро, подходит для использования даже без GPU.  
- Задача решается эффективно без привлечения тяжёлых трансформеров.  

